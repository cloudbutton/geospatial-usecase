{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NDVI Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import lithops\n",
    "import time\n",
    "import shutil\n",
    "import pylab\n",
    "import os\n",
    "import gc\n",
    "from rasterio.io import MemoryFile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from PIL import Image\n",
    "from lithops import Storage\n",
    "\n",
    "import cloudbutton_geospatial.s2froms3 as s2froms3\n",
    "from cloudbutton_geospatial.utils import notebook as notebook_utils\n",
    "from cloudbutton_geospatial.io_utils.ndvi import get_ndvi_params, ndvi_calculation, ndvi_tile_sentinel, get_subset_raster, lonlat_to_utm, get_poly_within\n",
    "from cloudbutton_geospatial.io_utils.plot import tiff_overview, plot_map\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the date interval in which tiles will be processed:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"daniel-lithops-geospatial\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from_date, to_date = notebook_utils.pick_date_range()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the tile's cloud percentage threshold:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "percentage = notebook_utils.pick_percentage_slider()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the area which delimites the tiles you want to process (left click to mark a point in the map, right click to erase current selection):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "map_region = notebook_utils.MapRegion()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "coords = []\n",
    "lats = []\n",
    "lons = []\n",
    "points = []\n",
    "\n",
    "for value in map_region.get_region()[:-1]:\n",
    "    coords.append(value)\n",
    "    lats.append(value[1])\n",
    "    lons.append(value[0])\n",
    "\n",
    "start_date = from_date.value  # Start date to search images\n",
    "end_date = to_date.value  # End date to search images\n",
    "what = ['B04', 'B08']  # What we want to download\n",
    "cc = percentage.value  # Minimum cloud cover on each image, 25 is 25%\n",
    "\n",
    "for lon, lat in zip(lons, lats):\n",
    "    points.append([lon, lat])\n",
    "    print([lon, lat], start_date, end_date, what, cc)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate longitude and latency of the intermediate zones"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def distance(origin, destination):\n",
    "    lat1, lon1 = origin\n",
    "    lat2, lon2 = destination\n",
    "    radius = 6371  # km\n",
    "\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = (math.sin(dlat / 2) * math.sin(dlat / 2) +\n",
    "         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *\n",
    "         math.sin(dlon / 2) * math.sin(dlon / 2))\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = radius * c\n",
    "\n",
    "    return d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i, p = 0, 0\n",
    "\n",
    "while i != len(points):\n",
    "    p = i + 1\n",
    "    \n",
    "    while p != len(points):\n",
    "        \n",
    "        dis = distance(points[i], points[p])\n",
    "        divisions = int(dis / 100)\n",
    "        \n",
    "        # If the zones are separated by more than 100 km, generate intermediate zones\n",
    "        if divisions > 0:\n",
    "            toSum = [(points[i][0] - points[p][0]) / (divisions + 1) , (points[i][1] - points[p][1]) / (divisions + 1)]\n",
    "            \n",
    "            while divisions != 0:\n",
    "                point = points[i][0] - (toSum[0] * divisions)\n",
    "                \n",
    "                # Not add duplicated lons/lats\n",
    "                if point not in lons:\n",
    "                    lons.append(points[i][0] - (toSum[0] * divisions))\n",
    "                    lats.append(points[i][1] - (toSum[1] * divisions))\n",
    "                    \n",
    "                divisions = divisions - 1\n",
    "        p = p + 1 \n",
    "    i = i + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Sentinel-2 packages from a indefinite number of cells"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scenes_f1 = []\n",
    "scenes_f2 = []\n",
    "\n",
    "for longitude, latency in zip(lons, lats):\n",
    "    try:\n",
    "        # Get scenes from intital date\n",
    "        f1 = s2froms3.get_scene_list(lon=longitude, lat=latency, start_date=start_date, end_date=start_date,\n",
    "        what=what, cloud_cover_le=cc)\n",
    "\n",
    "        # Get scenes from end date\n",
    "        f2 = s2froms3.get_scene_list(lon=longitude, lat=latency, start_date=end_date, end_date=end_date,\n",
    "        what=what, cloud_cover_le=cc)\n",
    "\n",
    "        # Not add duplicated scenes\n",
    "        if len(scenes_f1) == 0 or f1 not in scenes_f1:\n",
    "            scenes_f1.append(f1)\n",
    "            scenes_f2.append(f2)\n",
    "\n",
    "            print(f'Found scenes {start_date}:', f1)\n",
    "            print(f'Found scenes {end_date}:', f2)\n",
    "            print(f'Lon: {longitude}, Lat: {latency}')\n",
    "            print(f'Cell: {f1[0][0].split(\"/\")[2]} {f1[0][0].split(\"/\")[3]} {f1[0][0].split(\"/\")[4]}\\n')\n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "if len(scenes_f1) == 0:\n",
    "    raise Exception('No data found')\n",
    "\n",
    "scene = scenes_f1[-1][-1]\n",
    "scene_band = rasterio.open('s3://'+scene[0])\n",
    "windows = list(scene_band.block_windows())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parallel execution in the cloud"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_ndvi(scene, ij_window, storage):\n",
    "\n",
    "    ij, window = ij_window\n",
    "    band_4_s3_loc, band_8_s3_loc = scene\n",
    "    band_path = band_4_s3_loc.split('/')\n",
    "    ndvi_local = f'/tmp/{band_path[7]}_{ij}_NDVI.tif'\n",
    "    jpg_local = f'/tmp/{band_path[7]}_{ij}_NDVI.jpg'\n",
    "\n",
    "    # generate nir and red objects as arrays in float64 format\n",
    "    # Read from S3 to lambda\n",
    "\n",
    "    get_t0 = time.time()\n",
    "    band4 = rasterio.open('s3://'+band_4_s3_loc)  # red\n",
    "    band8 = rasterio.open('s3://'+band_8_s3_loc)  # nir\n",
    "\n",
    "\n",
    "    profile = band4.profile\n",
    "    profile.update(dtype='float64')\n",
    "    profile.update(width=window.width)\n",
    "    profile.update(height=window.height)\n",
    "\n",
    "    with rasterio.open(ndvi_local, 'w', **profile) as dst:\n",
    "        red = band4.read(1, window=window).astype('float64')\n",
    "        nir = band8.read(1, window=window).astype('float64')\n",
    "        get_t1 = time.time()\n",
    "        ndvi = (np.where((nir + red) == 0., 0, (nir - red) / (nir + red))).astype('float64')\n",
    "        ndvi_mean = np.mean(ndvi, axis=0)\n",
    "        dst.write(ndvi, 1)\n",
    "        ndvi[0][0] = -1\n",
    "        ndvi[0][1] = 1\n",
    "        plt.imsave(jpg_local, ndvi, cmap=\"RdYlGn\")\n",
    "\n",
    "    # THROUGHPUT WRITE\n",
    "    with open(jpg_local, 'rb') as jpg_temp:\n",
    "        put_t0 = time.time()\n",
    "        co_jpg = storage.put_cloudobject(jpg_temp.read(), key=jpg_local.replace('/tmp/', ''))\n",
    "        put_t1 = time.time()\n",
    "\n",
    "        obj_metadata = storage.head_object(BUCKET_NAME, jpg_local.replace('/tmp/', ''))\n",
    "        put_sz = float(obj_metadata.get('content-length'))\n",
    "\n",
    "    write_bandwidth_mb = put_sz / (put_t1 - put_t0) / 1e6\n",
    "\n",
    "    stats = {\n",
    "        'put': {'t0': put_t0, 't1': put_t1, 'bandwidth': write_bandwidth_mb, 'size': put_sz},\n",
    "        'get': {'t0': get_t0, 't1': get_t1, 'bandwidth': 0, 'size': 0},\n",
    "    }\n",
    "\n",
    "    return ndvi_local, ndvi_mean, co_jpg, stats\n",
    "\n",
    "\n",
    "def compute_ndvi_diff(old_scene, new_scene, ij_window, storage):\n",
    "\n",
    "    ij, window = ij_window\n",
    "    band_path = new_scene[0].split('/')\n",
    "    jpg_diff_local = f'/tmp/{band_path[7]}_{ij}_NDVI_DIFF.jpg'\n",
    "\n",
    "    ndvi_local_f1, ndvi_mean_f1, co_jpg_f1, stats_f1 = calculate_ndvi(old_scene, ij_window, storage)\n",
    "    ndvi_local_f2, ndvi_mean_f2, co_jpg_f2, stats_f2 = calculate_ndvi(new_scene, ij_window, storage)\n",
    "\n",
    "    ndvi_old = rasterio.open(ndvi_local_f1)\n",
    "    ndvi_new = rasterio.open(ndvi_local_f2)\n",
    "\n",
    "    profile = ndvi_old.profile\n",
    "    profile.update(dtype='float64')\n",
    "    profile.update(width=window.width)\n",
    "    profile.update(height=window.height)\n",
    "\n",
    "    no = ndvi_old.read(1).astype('float64')\n",
    "    nn = ndvi_new.read(1).astype('float64')\n",
    "    ndvi_cmp = ((nn - no) * (nn + no)).astype('float64')\n",
    "    ndvi_cmp[0][0] = -1\n",
    "    ndvi_cmp[0][1] = 1\n",
    "    plt.imsave(jpg_diff_local, ndvi_cmp, cmap=\"RdYlGn\")\n",
    "\n",
    "    with open(jpg_diff_local, 'rb') as jpg_diff_file:\n",
    "        co_jpg_diff = storage.put_cloudobject(jpg_diff_file, key=jpg_diff_local.replace('/tmp/', ''))\n",
    "\n",
    "    return ij_window, co_jpg_f1, co_jpg_f2, co_jpg_diff, stats_f1, stats_f2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using the selected parameters, get the identifiers of the selected tiles from Sentinel-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fexec = lithops.FunctionExecutor(\n",
    "        backend='aws_lambda',\n",
    "        storage='aws_s3',\n",
    "        log_level='DEBUG',\n",
    "        monitoring='rabbitmq',\n",
    "        #max_workers=10,\n",
    "        runtime='aws_lambda/lithops-ndvi-v39:01'  # Runtime for AWS Lambda\n",
    ")\n",
    "\n",
    "# Get data from all cells\n",
    "iterdata = []\n",
    "for scene_f1, scene_f2 in zip(scenes_f1, scenes_f2):\n",
    "    for wd in windows:\n",
    "        iterdata.append((scene_f1[0], scene_f2[0], wd))\n",
    "\n",
    "# Execution\n",
    "fexec.map(compute_ndvi_diff, iterdata)\n",
    "results = fexec.get_result()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate plots\n",
    "fexec.plot(dst='C:\\\\Users\\\\alega\\\\PycharmProjects\\\\geospatial-usecase\\\\ndvi-diff\\\\lithops' + str([lat, lon]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate throughput statistics\n",
    "def create_agg_bdwth_plot(res_write, res_read, dst):\n",
    "    def compute_times_rates(start_time, d):\n",
    "        x = np.array(d)\n",
    "        tzero = start_time\n",
    "        tr_start_time = x[:, 0] - tzero\n",
    "        tr_end_time = x[:, 1] - tzero\n",
    "        rate = x[:, 2]\n",
    "\n",
    "        N = len(tr_start_time)\n",
    "        runtime_rate_hist = np.zeros((N, len(runtime_bins)))\n",
    "\n",
    "        for i in range(N):\n",
    "            s = tr_start_time[i]\n",
    "            e = tr_end_time[i]\n",
    "            a, b = np.searchsorted(runtime_bins, [s, e])\n",
    "            if b-a > 0:\n",
    "                runtime_rate_hist[i, a:b] = rate[i]\n",
    "\n",
    "        return {'start_time': tr_start_time,\n",
    "                'end_time': tr_end_time,\n",
    "                'rate': rate,\n",
    "                'runtime_rate_hist': runtime_rate_hist}\n",
    "    \n",
    "    start_time = min((min(t['t0'] for t in res_write), (min(t['t0'] for t in res_read)))) - 1\n",
    "\n",
    "    fig = pylab.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    for datum, l in [(res_write, 'Aggregate Write Bandwidth'), (res_read, 'Aggregate Read Bandwidth')]:\n",
    "        mb_rates = [(res['t0'], res['t1'], res['bandwidth']) for res in datum]\n",
    "        max_seconds = int(max([mr[1]-start_time for mr in mb_rates])*1.2)\n",
    "        max_seconds = 8 * round(max_seconds/8)\n",
    "        runtime_bins = np.linspace(0, max_seconds, max_seconds)\n",
    "\n",
    "        mb_rates_hist = compute_times_rates(start_time, mb_rates)\n",
    "\n",
    "        ax.plot(mb_rates_hist['runtime_rate_hist'].sum(axis=0)/1000, label=l)\n",
    "\n",
    "    ax.set_xlabel('Execution Time (sec)')\n",
    "    ax.set_ylabel(\"GB/sec\")\n",
    "    ax.set_xlim(0, )\n",
    "    ax.set_ylim(0, )\n",
    "    pylab.legend()\n",
    "    pylab.grid(True, axis='y')\n",
    "\n",
    "    dst = os.path.expanduser(dst) if '~' in dst else dst\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(dst, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare read data size \n",
    "get_sz = 0\n",
    "for scene in scenes_f1:\n",
    "    obj = storage.head_object('sentinel-cogs', scene[0][0].replace('sentinel-cogs/', ''))\n",
    "    obj2 = storage.head_object('sentinel-cogs', scene[0][1].replace('sentinel-cogs/', ''))\n",
    "    get_sz = get_sz + float(obj.get('content-length')) + float(obj2.get('content-length'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare throughput data\n",
    "list_throughput = []\n",
    "for worker in results:\n",
    "    list_throughput.append(worker[4])\n",
    "    list_throughput.append(worker[5])\n",
    "\n",
    "read_results = [stat['get'] for stat in list_throughput]\n",
    "write_results = [stat['put'] for stat in list_throughput]\n",
    "\n",
    "\n",
    "# Calculate throughput write data\n",
    "size_total_write = 0\n",
    "for value in write_results:\n",
    "    size_total_write = size_total_write + value.get('size')\n",
    "    \n",
    "# Calculate throughput read data\n",
    "size_total_read = 0\n",
    "for value in read_results:\n",
    "    size_per_worker = get_sz / numberWorkers\n",
    "    size_total_read = size_total_read + size_per_worker\n",
    "    value['size'] = size_per_worker\n",
    "    value['bandwidth'] = size_per_worker / (value.get('t1') - value.get('t0')) / 1e6\n",
    "\n",
    "    \n",
    "# Throughput read numeric:\n",
    "throughput_interpolation_read = size_total_read / duration  # Bytes/second\n",
    "\n",
    "# Throughput write numeric:\n",
    "throughput_interpolation_write = size_total_write / duration  # Bytes/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare duration data: \n",
    "startTime = set()\n",
    "endTime = set()\n",
    "\n",
    "for future in fexec.futures:\n",
    "    for key in future.stats.keys():\n",
    "        if key.endswith(\"worker_func_start_tstamp\"):\n",
    "            startTime.add(future.stats[key])\n",
    "        if key.endswith(\"worker_end_tstamp\"):\n",
    "            endTime.add(future.stats[key])\n",
    "            \n",
    "            \n",
    "duration = max(endTime) - min(startTime)\n",
    "\n",
    "\n",
    "# Prepare number of workers:\n",
    "numberWorkers=len(fexec.futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate throughput graphic \n",
    "create_agg_bdwth_plot(read_results, write_results, fexec.executor_id + '-storage-kpi.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print results:\n",
    "print(f\"Data size bytes: {get_sz}\")\n",
    "mb = get_sz / 2**20\n",
    "print(f\"Data size megabytes: {mb}\")\n",
    "gb = mb / 1000\n",
    "print(f\"Data size gigabytes: {gb}\")\n",
    "\n",
    "print(f\"Number of workers: {numberWorkers}\")\n",
    "\n",
    "print(f\"Throughput write: {throughput_interpolation_write / 1024**2} MiB/s\")\n",
    "print(f\"Throughput read: {throughput_interpolation_read / 1024**2} MiB/s\")\n",
    "\n",
    "print(f\"Duration: {duration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get and plot the computed jpg diff tile image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_jpg(data):\n",
    "    file = '_'.join(data[0][1].key.split('_')[:5])\n",
    "    \n",
    "    if 'DIFF' in data[0][1].key:\n",
    "        out_file = f'AwsData/{file}_NDVI_DIFF.jpg'\n",
    "    else:\n",
    "        out_file = f'AwsData/{file}_NDVI.jpg'\n",
    "        \n",
    "    jpgs = {}\n",
    "\n",
    "    def get_window(data):\n",
    "        ij_window, co_jpg = data\n",
    "        row = ij_window[0][0]\n",
    "        col = ij_window[0][1]\n",
    "        jpg_stream = fexec.storage.get_cloudobject(co_jpg, stream=True)\n",
    "\n",
    "        if row not in jpgs:\n",
    "            jpgs[row] = [None]*11\n",
    "\n",
    "        jpgs[row][col] = Image.open(jpg_stream)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=len(data)) as ex:\n",
    "        fs = ex.map(get_window, data)\n",
    "\n",
    "    # OJO CON EL SCENE_BAND PORQUE ESTO ESTA HECHO PARA 1 SOLA CELDA\n",
    "    new_im = Image.new('RGB', (scene_band.width, scene_band.height))\n",
    "\n",
    "    x_offset = 0\n",
    "    y_offset = 0\n",
    "\n",
    "    for row in sorted(jpgs.keys()):\n",
    "        for im in jpgs[row]:\n",
    "            new_im.paste(im, (x_offset, y_offset))\n",
    "            x_offset += im.size[0]\n",
    "        x_offset = 0\n",
    "        y_offset += im.size[1]\n",
    "        \n",
    "    thumbnail_zise = (640, 640)\n",
    "    new_im.thumbnail(thumbnail_zise)\n",
    "\n",
    "    #fig = plt.figure(figsize=(10, 10))\n",
    "    #plt.title(out_file)\n",
    "    #plt.imshow(new_im)\n",
    "    images[out_file] = new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "co_jpgs_f1 = [(res[0], res[1]) for res in results]\n",
    "co_jpgs_f2 = [(res[0], res[2]) for res in results]\n",
    "co_jpgs_diff = [(res[0], res[3]) for res in results]\n",
    "\n",
    "images = {}\n",
    "with ThreadPoolExecutor(max_workers=3) as ex:\n",
    "    fs = ex.map(get_jpg, [co_jpgs_f1, co_jpgs_f2, co_jpgs_diff])\n",
    "\n",
    "f, ax = plt.subplots(1,3, figsize=(18, 18))\n",
    "i = 0\n",
    "for j in sorted(images.keys()):\n",
    "    ax[i].set_title(j)\n",
    "    ax[i].imshow(images[j])\n",
    "    i = i+1\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Delete temporal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "keys = storage.list_keys(bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    storage.delete_object(bucket=BUCKET_NAME, key=key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudbutton-geospatial",
   "language": "python",
   "name": "cloudbutton-geospatial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}