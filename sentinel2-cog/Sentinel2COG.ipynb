{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloudbutton Geospatial Use Case: Sentinel2 Satellital Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "import sentinelsat\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import lithops\n",
    "import random\n",
    "import shutil\n",
    "import rasterio\n",
    "import re\n",
    "import tempfile\n",
    "import zipfile\n",
    "import subprocess\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import json\n",
    "from rio_cogeo import cogeo\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "from fiona.io import ZipMemoryFile\n",
    "from matplotlib import pyplot as plt\n",
    "from rasterio.io import MemoryFile\n",
    "from zipfile import ZipFile\n",
    "from ipyleaflet import Map, basemaps, basemap_to_tiles\n",
    "from lithops import Storage\n",
    "\n",
    "from cloudbutton_geospatial.utils import notebook as notebook_utils\n",
    "from cloudbutton_geospatial.io_utils.ndvi import get_ndvi_params, ndvi_calculation, ndvi_tile_sentinel, get_subset_raster, lonlat_to_utm, get_poly_within\n",
    "from cloudbutton_geospatial.io_utils.plot import tiff_overview, plot_map\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the environmental variables *SENTINEL_USERNAME* and *SENTINEL_PASSWORD* to match your Sentinel-2 credentials. You can register and access data for free at https://sentinel.esa.int/web/sentinel/sentinel-data-access/registration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTINEL_USERNAME = ''\n",
    "SENTINEL_PASSWORD = ''\n",
    "STORAGE_BACKEND = 'aws_s3'\n",
    "BATCH_BACKEND = 'aws_batch'\n",
    "BATCH_RUNTIME = 'cloudbutton-geospatial-sentinel:01'\n",
    "FAAS_BACKEND = 'aws_lambda'\n",
    "FAAS_RUNTIME = ''\n",
    "STORAGE_BUCKET = 'cloudbutton-geospatial-sentinel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_storage = Storage(backend=STORAGE_BACKEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the date interval in which tiles will be processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_day, to_day = notebook_utils.pick_date_range()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the tile's cloud percentage threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = notebook_utils.pick_percentage_slider()\n",
    "from_day.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the area which delimites the tiles you want to process (left click to mark a point in the map, right click to erase current selection):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_region = notebook_utils.MapRegion(zoom=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sentinel-2 metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# locations = map_region.get_region()\n",
    "\n",
    "# Tarragona data\n",
    "locations = [[1.5481363013595886, 41.16508628430497],\n",
    " [0.8177319989996914, 40.62111912603713],\n",
    " [0.6008074129604647, 40.60652433834119],\n",
    " [0.4552757286556909, 40.868742532626996],\n",
    " [0.3811369460853299, 41.03883697553436],\n",
    " [0.427816920296289, 41.247740935856484],\n",
    " [0.694167361382423, 41.33441592882952],\n",
    " [1.097811844265526, 41.39831645175795],\n",
    " [1.452030472101722, 41.365343372983396],\n",
    " [1.5481363013595886, 41.16508628430497]]\n",
    "\n",
    "\n",
    "# debug\n",
    "#locations = [[-1.32110595703125, 37.57329031970199],\n",
    "#   [-2.0681762695312504, 37.684227882053044],\n",
    "#   [-1.636962890625, 38.24289903439589],\n",
    "#   [-0.7745361328125, 38.12199840979802],\n",
    "#   [-1.32110595703125, 37.57329031970199]]\n",
    "\n",
    "print(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_json_area = {\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"geometry\": {\n",
    "                \"coordinates\":[locations],\n",
    "                \"type\": \"Polygon\"\n",
    "            },\n",
    "            \"properties\": {},\n",
    "            \"type\": \"Feature\"\n",
    "        }\n",
    "    ],\n",
    "    \"type\": \"FeatureCollection\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the selected parameters, get the identifiers of the selected tiles from Sentinel-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentinel_api = sentinelsat.SentinelAPI(user=SENTINEL_USERNAME,\n",
    "                                       password=SENTINEL_PASSWORD,\n",
    "                                       show_progressbars=False)\n",
    "footprint = sentinelsat.geojson_to_wkt(geo_json_area)\n",
    "products = sentinel_api.query(footprint,\n",
    "                              date=(from_day.value, to_day.value),\n",
    "                              platformname='Sentinel-2',\n",
    "                              producttype=('S2MS2Ap', 'S2MSI1C'),\n",
    "                              cloudcoverpercentage=(0, percentage.value))\n",
    "tiles_ids = []\n",
    "products_ids = []\n",
    "\n",
    "for product in list(products.keys()):\n",
    "    product_id = products[product]['identifier']\n",
    "    tile_id = products[product]['level1cpdiidentifier']\n",
    "    tiles_ids.append(tile_id)\n",
    "    products_ids.append(product_id)\n",
    "\n",
    "geojson_products = sentinel_api.to_geojson(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of tiles: {}'.format(len(geojson_products['features'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sentinelhub import AwsProductRequest\n",
    "#from sentinelhub import AwsTile\n",
    "\n",
    "#product_id = 'S2A_MSIL1C_20210902T105031_N0301_R051_T31TBE_20210902T130744'\n",
    "#data_folder = './AwsData'\n",
    "#request = AwsProductRequest(product_id=product_id, data_folder=data_folder, safe_format=True)\n",
    "\n",
    "# Uncomment the the following line to download the data:\n",
    "#data_list = request.get_data(save_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(product_id, product_meta, storage):\n",
    "    \n",
    "    product = {product_id: product_meta}\n",
    "    storage_prefix = 'sentinel'\n",
    "    tiles_in_storage = storage.list_keys(bucket=STORAGE_BUCKET, prefix=storage_prefix)\n",
    "    \n",
    "    tile_id = products[product_id]['identifier']\n",
    "    product_storage_key = os.path.join(storage_prefix, tile_id+'.zip')\n",
    "    if product_storage_key in tiles_in_storage:\n",
    "        return tile_id\n",
    "\n",
    "    sentinel_api = sentinelsat.SentinelAPI(user=os.environ[\"SENTINEL_USERNAME\"],\n",
    "                                           password=os.environ[\"SENTINEL_PASSWORD\"],\n",
    "                                           show_progressbars=False)\n",
    "    \n",
    "    tmpdir = tempfile.gettempdir()\n",
    "    sentinel_api.download_all(product, directory_path=tmpdir)\n",
    "\n",
    "    tile_id = products[product_id]['identifier']\n",
    "    print(f\"Tile name: {tile_id}\")\n",
    "    product_local_file = os.path.join(tmpdir, tile_id+'.zip')\n",
    "    product_storage_key = os.path.join(storage_prefix, tile_id+'.zip')\n",
    "    print(f\"Uploading tile {tile_id} to Storage\")\n",
    "    with open(product_local_file, 'rb') as tiffile:\n",
    "        storage.put_object(bucket=STORAGE_BUCKET, key=product_storage_key, body=tiffile)\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterdata = []\n",
    "\n",
    "for product_id, product_meta in products.items():\n",
    "    iterdata.append((product_id, product_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_env = {'SENTINEL_USERNAME': SENTINEL_USERNAME,\n",
    "             'SENTINEL_PASSWORD': SENTINEL_PASSWORD}\n",
    "\n",
    "fexec = lithops.FunctionExecutor(backend=BATCH_BACKEND, storage=STORAGE_BACKEND, runtime=BATCH_RUNTIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be sequential\n",
    "dw_data_fs = []\n",
    "for product in iterdata:\n",
    "    fut = fexec.call_async(download_dataset, product, extra_env=extra_env)\n",
    "    fexec.wait(fs=fut)\n",
    "    dw_data_fs.append(fut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Athmospheric correction using Serverful Lithops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will download tile images from Sentinel2 using the previously selected configuration and apply athmospheric correction.\n",
    "\n",
    "This process is not parallelizable and lasts for over 25 minutes, so it is not suited for serverless functions. We will use Lithops Standalone instead, which uses serverful instances that haven't time limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jp2_to_cog(band_src_path):\n",
    "    \"\"\"\n",
    "    Transform a sentinel2 band (.jp2) to GeoTiff (.tif)\n",
    "    \"\"\"\n",
    "    config = dict(NUM_THREADS=100, GDAL_TIFF_OVR_BLOCKSIZE=128)\n",
    "\n",
    "    output_profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"interleave\": \"pixel\",\n",
    "        \"tiled\": True,\n",
    "        \"blockxsize\": 256,\n",
    "        \"blockysize\": 256,\n",
    "        \"compress\": \"DEFLATE\",\n",
    "    }\n",
    "\n",
    "    cog_path = f\"{band_src_path[band_src_path.rfind('/')+1:band_src_path.rfind('.')]}.tif\"\n",
    "    cogeo.cog_translate(\n",
    "        band_src_path,\n",
    "        cog_path,\n",
    "        output_profile,\n",
    "        nodata=0,\n",
    "        in_memory=False,\n",
    "        config=config,\n",
    "        quiet=True,\n",
    "    )\n",
    "\n",
    "    return cog_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_atmospheric_correction(product_geojson, storage):\n",
    "    product = product_geojson['properties']\n",
    "    tile = product['filename'][39:44]\n",
    "    date = product['filename'][11:19]\n",
    "\n",
    "    tmpdir = tempfile.gettempdir()\n",
    "    \n",
    "    product_local_file = os.path.join(tmpdir, product['identifier']+'.zip')\n",
    "    product_storage_key = os.path.join('sentinel', product['identifier']+'.zip')\n",
    "    \n",
    "    if not os.path.isfile(product_local_file):\n",
    "        print(f'Copying {product_storage_key} to local disk')\n",
    "        obj_stream = storage.get_object(bucket=STORAGE_BUCKET, key=product_storage_key, stream=True)    \n",
    "        with open(product_local_file, 'wb') as shapf:\n",
    "            shutil.copyfileobj(obj_stream, shapf)\n",
    "        print(f'Finished copying {product_storage_key} to local disk')\n",
    " \n",
    "    # Extract and remove zip file\n",
    "    print('Extracting zip file')\n",
    "    zip_ref = zipfile.ZipFile(product_local_file)\n",
    "    zip_ref.extractall(tmpdir)\n",
    "    zip_ref.close()\n",
    "\n",
    "    # Atmospheric correction\n",
    "    print('Starting atmospheric correction')\n",
    "    sentinel_product_dir = os.path.join(tmpdir, product['filename'])\n",
    "    corrected_images = glob.glob(f\"*2A_{date}*_T{tile}_*.SAFE/GRANULE/*/IMG_DATA/R10m/*B0[48]*.jp2\")\n",
    "    atmospheric_corrected = corrected_images[0] if len(corrected_images) > 0 else None\n",
    "\n",
    "    if not atmospheric_corrected:\n",
    "        print(f'Doing the atmospheric correction for {sentinel_product_dir}')\n",
    "        retry = 0\n",
    "        while True:\n",
    "            try:\n",
    "                cmd = ['L2A_Process --resolution 10 {}'.format(sentinel_product_dir)]\n",
    "                val = subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True, universal_newlines=True)\n",
    "                corrected_images = glob.glob(f\"*2A_{date}*_T{tile}_*.SAFE/GRANULE/*/IMG_DATA/R10m/*B0[48]*.jp2\")\n",
    "                print(f'Atmospheric correction finished {val}')\n",
    "                break\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                if retry<3:\n",
    "                    retry += 1\n",
    "                    time.sleep(2)\n",
    "                else:   \n",
    "                    print(e.returncode)\n",
    "                    print(e.output)\n",
    "                    raise(e)\n",
    "\n",
    "\n",
    "    # Translate bands in .jp2 to GeoTiff format\n",
    "    band_files = []\n",
    "    band4 = glob.glob(os.path.join(tmpdir, '*L2A_{}*_T{}*.SAFE/GRANULE/*/IMG_DATA/R10m/*B04*'.format(date, tile))).pop()\n",
    "    band8 = glob.glob(os.path.join(tmpdir, '*L2A_{}*_T{}*.SAFE/GRANULE/*/IMG_DATA/R10m/*B08*'.format(date, tile))).pop()\n",
    "\n",
    "    if band4 is not None and band8 is not None:\n",
    "        band4_tiff_file = f\"{band4[band4.rfind('/')+1:band4.rfind('.')]}.tif\"\n",
    "        band8_tiff_file = f\"{band8[band8.rfind('/') + 1:band8.rfind('.')]}.tif\"\n",
    "        jp2_to_cog(band4)\n",
    "        jp2_to_cog(band8)\n",
    "        band_files.append(band4_tiff_file)\n",
    "        band_files.append(band8_tiff_file)\n",
    "    \n",
    "    print(band_files)\n",
    "\n",
    "    # Merge both bands into a single geotiff\n",
    "    combined_geotiff_key = band_files[0][0:22] + '_COMBINED.tif'\n",
    "    with rasterio.open(band_files[0]) as src:\n",
    "        profile = src.profile\n",
    "        profile.update(count=len(band_files))\n",
    "\n",
    "    with rasterio.open(combined_geotiff_key, 'w', **profile) as dst:\n",
    "        for i, band_file in enumerate(band_files):\n",
    "            with rasterio.open(band_file) as src:\n",
    "                dst.write(src.read(1), i + 1)\n",
    "\n",
    "    # Upload generated files to Cloud Storage\n",
    "    with open(combined_geotiff_key, 'rb') as combined_geotiff_f:\n",
    "        storage.put_object(Bucket=STORAGE_BUCKET, Key=combined_geotiff_key, Body=combined_geotiff_f)\n",
    "    product_meta_key = combined_geotiff_key + '.meta.json'\n",
    "    storage.put_object(Bucket=STORAGE_BUCKET, Key=product_meta_key, Body=json.dumps(product))\n",
    "\n",
    "    return combined_geotiff_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(geojson_products[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fexec = lithops.FunctionExecutor(backend='aws_batch', storage='aws_s3', runtime=BATCH_RUNTIME)\n",
    "\n",
    "fexec.map(perform_atmospheric_correction, geojson_products[\"features\"])\n",
    "\n",
    "combined_keys = fexec.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDVI Computation using Serverless Lithops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate NDVI index of tiles tha thave been downloaded and pre-processed before.\n",
    "\n",
    "This process can be executed in parallel (for every tile) and in serverless functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# combined_keys = ['T30SXG_20201229T110451_COMBINED.tif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndvi(combined_key, storage):\n",
    "    tmpdir = tempfile.gettempdir()\n",
    "    dat = storage.get_object(bucket=STORAGE_BUCKET, key=combined_key, stream=True)\n",
    "    out = os.path.join(tmpdir, 'out.tif')\n",
    "\n",
    "    with rasterio.open(dat) as src:\n",
    "        profile = src.profile\n",
    "        profile.update(dtype='float32')\n",
    "        profile.update(count=1)\n",
    "        with rasterio.open(out, 'w', **profile) as dst:\n",
    "            for _, window in src.block_windows(1):\n",
    "                red = src.read(1, window=window).astype('float32')\n",
    "                nir = src.read(2, window=window).astype('float32')\n",
    "                ndvi = (np.where((nir + red) == 0., 0,\n",
    "                                 (nir - red) / (nir + red))).astype('float32')\n",
    "                dst.write(ndvi, 1, window=window)\n",
    "\n",
    "    prefix = combined_key.rsplit('_', 1)[0]\n",
    "    output_key = prefix + '_NDVI.tif'\n",
    "    with open(out, 'rb') as output_f:\n",
    "        storage.put_object(bucket=STORAGE_BUCKET, key=output_key, body=output_f)\n",
    "\n",
    "    return output_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fexec = lithops.FunctionExecutor(backend=COMPUTE_BACKEND, storage=STORAGE_BACKEND,\n",
    "                                 runtime=RUNTIME, runtime_memory=2048, log_level='DEBUG')\n",
    "fexec.map(ndvi, combined_keys, timeout=60)\n",
    "ndvi_keys = fexec.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# ndvi_keys = ['T30SXG_20201229T110451_NDVI.tif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_select = notebook_utils.pick_tile(ndvi_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = cloud_storage.get_object(bucket=STORAGE_BUCKET, key=tile_select.value, stream=True)\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(20,15))\n",
    "\n",
    "with rasterio.open(obj) as src:\n",
    "#     ij, window = random.choice(list(src.block_windows()))\n",
    "#     arr = src.read(1, window=window)\n",
    "    arr = src.read(1)\n",
    "    plt.imshow(arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
